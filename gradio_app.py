#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CosyVoice Gradio Web App
ë¸Œë¼ìš°ì €ì—ì„œ ìŒì„± ë…¹ìŒ â†’ STT â†’ TTS ìƒì„±ì„ ìœ„í•œ ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import sys
import os
import gradio as gr
import torch
import torchaudio
import whisper
import numpy as np
import tempfile
from pathlib import Path
import logging

# CosyVoice ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))
sys.path.append(str(current_dir / "third_party" / "Matcha-TTS"))

try:
    from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
    from cosyvoice.utils.file_utils import load_wav
    COSYVOICE_AVAILABLE = True
except ImportError as e:
    print(f"CosyVoice import error: {e}")
    COSYVOICE_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CosyVoiceGradioApp:
    def __init__(self):
        self.whisper_model = None
        self.cosyvoice_model = None
        self.current_model_path = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
    
    def load_whisper(self, model_size="base"):
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        if self.whisper_model is None:
            logger.info(f"Loading Whisper model: {model_size}")
            self.whisper_model = whisper.load_model(model_size)
        return self.whisper_model
    
    def load_cosyvoice(self, model_path):
        """CosyVoice ëª¨ë¸ ë¡œë“œ"""
        if not COSYVOICE_AVAILABLE:
            raise Exception("CosyVoiceê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.current_model_path != model_path:
            logger.info(f"Loading CosyVoice model: {model_path}")
            try:
                if "CosyVoice2" in model_path:
                    self.cosyvoice_model = CosyVoice2(model_path, 
                                                     load_jit=False, 
                                                     load_trt=False, 
                                                     fp16=False)
                else:
                    self.cosyvoice_model = CosyVoice(model_path, 
                                                    load_jit=False, 
                                                    load_trt=False, 
                                                    fp16=False)
                self.current_model_path = model_path
                logger.info("CosyVoice model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load CosyVoice: {e}")
                raise
        
        return self.cosyvoice_model
    
    def transcribe_audio(self, audio_file, whisper_size="base", language="auto"):
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            if audio_file is None:
                return "âŒ ìŒì„± íŒŒì¼ì„ ë¨¼ì € ë…¹ìŒí•´ì£¼ì„¸ìš”."
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            model = self.load_whisper(whisper_size)
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
            if isinstance(audio_file, str):
                # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                audio_path = audio_file
            else:
                # íŠœí”Œì¸ ê²½ìš° (sample_rate, audio_data)
                sample_rate, audio_data = audio_file
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    # ì •ê·œí™”
                    if audio_data.dtype == np.int16:
                        audio_data = audio_data.astype(np.float32) / 32768.0
                    elif audio_data.dtype == np.int32:
                        audio_data = audio_data.astype(np.float32) / 2147483648.0
                    
                    # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
                    if sample_rate != 16000:
                        import librosa
                        audio_data = librosa.resample(audio_data, 
                                                    orig_sr=sample_rate, 
                                                    target_sr=16000)
                    
                    torchaudio.save(tmp_file.name, 
                                  torch.from_numpy(audio_data).unsqueeze(0), 
                                  16000)
                    audio_path = tmp_file.name
            
            # STT ì‹¤í–‰
            if language == "auto":
                result = model.transcribe(audio_path)
            else:
                result = model.transcribe(audio_path, language=language)
            
            transcribed_text = result["text"].strip()
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'tmp_file' in locals():
                os.unlink(audio_path)
            
            logger.info(f"Transcription result: {transcribed_text}")
            return f"âœ… ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcribed_text}"
            
        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return f"âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}"
    
    def generate_speech(self, reference_audio, prompt_text, target_text, 
                       model_path, mode="zero_shot", speaker_id="ì¤‘ë¬¸ì—¬"):
        """ìŒì„± ìƒì„±"""
        try:
            if not target_text.strip():
                return None, "âŒ ìƒì„±í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            # CosyVoice ëª¨ë¸ ë¡œë“œ
            model = self.load_cosyvoice(model_path)
            
            if mode == "sft":
                # SFT ëª¨ë“œ - ë¯¸ë¦¬ í•™ìŠµëœ í™”ì ì‚¬ìš©
                output_audio = None
                for i, result in enumerate(model.inference_sft(target_text, speaker_id, stream=False)):
                    output_audio = result['tts_speech'].cpu().numpy()
                    break
                
                if output_audio is not None:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                        torchaudio.save(tmp_file.name, 
                                      torch.from_numpy(output_audio), 
                                      model.sample_rate)
                        return tmp_file.name, "âœ… ìŒì„±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ (SFT ëª¨ë“œ)"
            
            elif mode == "zero_shot":
                # Zero-shot ëª¨ë“œ - ì°¸ì¡° ìŒì„± í•„ìš”
                if reference_audio is None:
                    return None, "âŒ Zero-shot ëª¨ë“œì—ì„œëŠ” ì°¸ì¡° ìŒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤."
                
                if not prompt_text.strip():
                    return None, "âŒ ì°¸ì¡° ìŒì„±ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
                
                # ì°¸ì¡° ìŒì„± ì²˜ë¦¬
                reference_speech = self.prepare_reference_audio(reference_audio)
                
                output_audio = None
                for i, result in enumerate(model.inference_zero_shot(
                    target_text, prompt_text, reference_speech, stream=False)):
                    output_audio = result['tts_speech'].cpu().numpy()
                    break
                
                if output_audio is not None:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                        torchaudio.save(tmp_file.name, 
                                      torch.from_numpy(output_audio), 
                                      model.sample_rate)
                        return tmp_file.name, "âœ… ìŒì„±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ (Zero-shot ëª¨ë“œ)"
            
            return None, "âŒ ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"Speech generation error: {e}")
            return None, f"âŒ ìŒì„± ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def prepare_reference_audio(self, audio_input):
        """ì°¸ì¡° ìŒì„±ì„ CosyVoice í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(audio_input, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            return load_wav(audio_input, 16000)
        else:
            # íŠœí”Œì¸ ê²½ìš° (sample_rate, audio_data)
            sample_rate, audio_data = audio_input
            
            # ì •ê·œí™” ë° ë³€í™˜
            if audio_data.dtype == np.int16:
                audio_data = audio_data.astype(np.float32) / 32768.0
            elif audio_data.dtype == np.int32:
                audio_data = audio_data.astype(np.float32) / 2147483648.0
            
            # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
            if sample_rate != 16000:
                import librosa
                audio_data = librosa.resample(audio_data, 
                                            orig_sr=sample_rate, 
                                            target_sr=16000)
            
            return torch.from_numpy(audio_data)
    
    def auto_generate(self, reference_audio, target_text, model_path, 
                     whisper_size="base", language="auto"):
        """ìë™ STT + TTS íŒŒì´í”„ë¼ì¸"""
        try:
            if reference_audio is None:
                return None, "âŒ ì°¸ì¡° ìŒì„±ì„ ë¨¼ì € ë…¹ìŒí•´ì£¼ì„¸ìš”."
            
            if not target_text.strip():
                return None, "âŒ ìƒì„±í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            # 1. STTë¡œ ì°¸ì¡° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            model = self.load_whisper(whisper_size)
            
            if isinstance(reference_audio, str):
                audio_path = reference_audio
            else:
                sample_rate, audio_data = reference_audio
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    if audio_data.dtype == np.int16:
                        audio_data = audio_data.astype(np.float32) / 32768.0
                    elif audio_data.dtype == np.int32:
                        audio_data = audio_data.astype(np.float32) / 2147483648.0
                    
                    if sample_rate != 16000:
                        import librosa
                        audio_data = librosa.resample(audio_data, 
                                                    orig_sr=sample_rate, 
                                                    target_sr=16000)
                    
                    torchaudio.save(tmp_file.name, 
                                  torch.from_numpy(audio_data).unsqueeze(0), 
                                  16000)
                    audio_path = tmp_file.name
            
            # STT ì‹¤í–‰
            if language == "auto":
                result = model.transcribe(audio_path)
            else:
                result = model.transcribe(audio_path, language=language)
            
            prompt_text = result["text"].strip()
            
            # 2. TTSë¡œ ìŒì„± ìƒì„±
            output_audio, message = self.generate_speech(
                reference_audio, prompt_text, target_text, model_path, "zero_shot"
            )
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'tmp_file' in locals():
                os.unlink(audio_path)
            
            if output_audio:
                return output_audio, f"âœ… ìë™ ìƒì„± ì™„ë£Œ!\nì¸ì‹ëœ í…ìŠ¤íŠ¸: '{prompt_text}'\nìƒì„±ëœ ìŒì„±ì„ í™•ì¸í•˜ì„¸ìš”."
            else:
                return None, f"âŒ ìë™ ìƒì„± ì‹¤íŒ¨: {message}"
            
        except Exception as e:
            logger.error(f"Auto generation error: {e}")
            return None, f"âŒ ìë™ ìƒì„± ì˜¤ë¥˜: {str(e)}"

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    app = CosyVoiceGradioApp()
    
    # ëª¨ë¸ ê²½ë¡œ ì˜µì…˜
    model_options = [
        "pretrained_models/CosyVoice2-0.5B",
        "pretrained_models/CosyVoice-300M",
        "pretrained_models/CosyVoice-300M-SFT",
        "pretrained_models/CosyVoice-300M-Instruct"
    ]
    
    with gr.Blocks(title="ğŸ™ï¸ CosyVoice ìŒì„± ë³µì œ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ™ï¸ CosyVoice ìŒì„± ë³µì œ ì‹œìŠ¤í…œ")
        gr.Markdown("ìŒì„±ì„ ë…¹ìŒí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´, ë‹¹ì‹ ì˜ ëª©ì†Œë¦¬ë¡œ ìŒì„±ì„ ìƒì„±í•©ë‹ˆë‹¤!")
        
        with gr.Tab("ğŸš€ ìë™ ìƒì„± (ì¶”ì²œ)"):
            gr.Markdown("### í•œ ë²ˆì— ìŒì„± ì¸ì‹ â†’ ìŒì„± ìƒì„±")
            
            with gr.Row():
                with gr.Column():
                    ref_audio_auto = gr.Audio(
                        label="ğŸ“¢ ì°¸ì¡° ìŒì„± ë…¹ìŒ", 
                        sources=["microphone"],
                        type="numpy"
                    )
                    target_text_auto = gr.Textbox(
                        label="ğŸ“ ìƒì„±í•  í…ìŠ¤íŠ¸",
                        placeholder="ì—¬ê¸°ì— ìƒì„±í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                        lines=3
                    )
                    
                    with gr.Row():
                        model_path_auto = gr.Dropdown(
                            choices=model_options,
                            value=model_options[0],
                            label="ğŸ¤– ëª¨ë¸ ì„ íƒ"
                        )
                        whisper_size_auto = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium"],
                            value="base",
                            label="ğŸ¯ Whisper ëª¨ë¸"
                        )
                    
                    generate_btn = gr.Button("ğŸµ ìŒì„± ìƒì„±í•˜ê¸°", variant="primary", size="lg")
                
                with gr.Column():
                    output_audio_auto = gr.Audio(label="ğŸ”Š ìƒì„±ëœ ìŒì„±")
                    status_auto = gr.Textbox(label="ğŸ“‹ ìƒíƒœ", interactive=False)
            
            generate_btn.click(
                fn=app.auto_generate,
                inputs=[ref_audio_auto, target_text_auto, model_path_auto, whisper_size_auto],
                outputs=[output_audio_auto, status_auto]
            )
        
        with gr.Tab("ğŸ”§ ìˆ˜ë™ ì„¤ì •"):
            gr.Markdown("### ë‹¨ê³„ë³„ ìŒì„± ìƒì„± (ê³ ê¸‰ ì‚¬ìš©ììš©)")
            
            with gr.Row():
                with gr.Column():
                    # STT ì„¹ì…˜
                    gr.Markdown("#### 1ï¸âƒ£ ìŒì„± ì¸ì‹ (STT)")
                    ref_audio_manual = gr.Audio(
                        label="ğŸ“¢ ì°¸ì¡° ìŒì„± ë…¹ìŒ", 
                        sources=["microphone"],
                        type="numpy"
                    )
                    
                    with gr.Row():
                        whisper_size_manual = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium"],
                            value="base",
                            label="Whisper í¬ê¸°"
                        )
                        language = gr.Dropdown(
                            choices=["auto", "ko", "en", "zh", "ja"],
                            value="auto",
                            label="ì–¸ì–´"
                        )
                    
                    transcribe_btn = gr.Button("ğŸ¯ ìŒì„± ì¸ì‹", variant="secondary")
                    transcribe_result = gr.Textbox(label="ì¸ì‹ ê²°ê³¼", interactive=False)
                    
                    # TTS ì„¹ì…˜
                    gr.Markdown("#### 2ï¸âƒ£ ìŒì„± ìƒì„± (TTS)")
                    prompt_text_manual = gr.Textbox(
                        label="ğŸ“ ì°¸ì¡° ìŒì„±ì˜ í…ìŠ¤íŠ¸",
                        placeholder="ìœ„ì˜ ì¸ì‹ ê²°ê³¼ë¥¼ ë³µì‚¬í•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥...",
                        lines=2
                    )
                    target_text_manual = gr.Textbox(
                        label="ğŸ“ ìƒì„±í•  í…ìŠ¤íŠ¸",
                        placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                        lines=3
                    )
                    
                    with gr.Row():
                        model_path_manual = gr.Dropdown(
                            choices=model_options,
                            value=model_options[0],
                            label="ğŸ¤– ëª¨ë¸"
                        )
                        mode = gr.Dropdown(
                            choices=["zero_shot", "sft"],
                            value="zero_shot",
                            label="ëª¨ë“œ"
                        )
                    
                    speaker_id = gr.Textbox(
                        label="í™”ì ID (SFT ëª¨ë“œìš©)",
                        value="ì¤‘ë¬¸ì—¬",
                        visible=False
                    )
                    
                    generate_manual_btn = gr.Button("ğŸµ ìŒì„± ìƒì„±", variant="primary")
                
                with gr.Column():
                    output_audio_manual = gr.Audio(label="ğŸ”Š ìƒì„±ëœ ìŒì„±")
                    status_manual = gr.Textbox(label="ğŸ“‹ ìƒíƒœ", interactive=False)
            
            # ëª¨ë“œ ë³€ê²½ ì‹œ í™”ì ID ì…ë ¥ í‘œì‹œ/ìˆ¨ê¹€
            def update_speaker_visibility(mode_value):
                return gr.update(visible=(mode_value == "sft"))
            
            mode.change(
                fn=update_speaker_visibility,
                inputs=[mode],
                outputs=[speaker_id]
            )
            
            # ì´ë²¤íŠ¸ ë°”ì¸ë”©
            transcribe_btn.click(
                fn=app.transcribe_audio,
                inputs=[ref_audio_manual, whisper_size_manual, language],
                outputs=[transcribe_result]
            )
            
            generate_manual_btn.click(
                fn=app.generate_speech,
                inputs=[ref_audio_manual, prompt_text_manual, target_text_manual, 
                       model_path_manual, mode, speaker_id],
                outputs=[output_audio_manual, status_manual]
            )
        
        with gr.Tab("â„¹ï¸ ì‚¬ìš©ë²•"):
            gr.Markdown("""
            ## ğŸ“– ì‚¬ìš©ë²•
            
            ### ğŸš€ ìë™ ìƒì„± ëª¨ë“œ (ì¶”ì²œ)
            1. **ì°¸ì¡° ìŒì„± ë…¹ìŒ**: ë§ˆì´í¬ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ 5-10ì´ˆê°„ ìŒì„±ì„ ë…¹ìŒí•˜ì„¸ìš”
            2. **í…ìŠ¤íŠ¸ ì…ë ¥**: ìƒì„±í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”
            3. **ëª¨ë¸ ì„ íƒ**: CosyVoice2-0.5Bë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤ (ì„±ëŠ¥ ìµœê³ )
            4. **ìƒì„± í´ë¦­**: 'ìŒì„± ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
            
            ### ğŸ”§ ìˆ˜ë™ ì„¤ì • ëª¨ë“œ
            1. **STT**: ì°¸ì¡° ìŒì„± ë…¹ìŒ â†’ ìŒì„± ì¸ì‹ í´ë¦­
            2. **ìˆ˜ì •**: ì¸ì‹ ê²°ê³¼ê°€ í‹€ë ¸ë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì •
            3. **TTS**: ìƒì„±í•  í…ìŠ¤íŠ¸ ì…ë ¥ â†’ ìŒì„± ìƒì„± í´ë¦­
            
            ## âš ï¸ ì£¼ì˜ì‚¬í•­
            - **ì°¸ì¡° ìŒì„±**: 5-10ì´ˆ, ëª…í™•í•œ ë°œìŒ, ë…¸ì´ì¦ˆ ìµœì†Œí™”
            - **ì§€ì› ì–¸ì–´**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´
            - **ëª¨ë¸ í¬ê¸°**: ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ ì†Œìš”
            
            ## ğŸ”§ ëª¨ë¸ ì„¤ëª…
            - **CosyVoice2-0.5B**: ìµœì‹  ëª¨ë¸, ì„±ëŠ¥ ìµœê³  (ì¶”ì²œ)
            - **CosyVoice-300M-SFT**: ë¯¸ë¦¬ í•™ìŠµëœ í™”ìë“¤ ì‚¬ìš©
            - **CosyVoice-300M-Instruct**: ê°ì •/ìŠ¤íƒ€ì¼ ì œì–´ ê°€ëŠ¥
            """)
    
    return demo

if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_dir = Path("pretrained_models")
    if not model_dir.exists():
        print("âš ï¸ pretrained_models í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("python -c \"from modelscope import snapshot_download; snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')\"")
    
    # Gradio ì•± ì‹¤í–‰
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        inbrowser=True
    )